Latency Improvement Dashboard + Mininet Light Mode (6 GB VM)

Date: 2025-09-10

1) Grafana/Prometheus: Latency Improvement Overview
- Dashboard UID: latency-improvement
- JSON: orchestration/grafana/dashboards/latency_improvement_dashboard.json
- Time range: Last 6 hours | Refresh: 10s
- Import JSON in Grafana; optional annotations for DRL updates

PromQL (key queries)
- P95 current: histogram_quantile(0.95, rate(sfc_current_latency_seconds_bucket[5m]))
- P95 baseline: vector(0.14)
- P95 improvement %: ((0.14 - histogram_quantile(0.95, rate(sfc_current_latency_seconds_bucket[5m]))) / 0.14) * 100
- P50/P95/P99/P99.9: histogram_quantile(0.50/0.95/0.99/0.999, rate(sfc_current_latency_seconds_bucket[5m]))
- Baselines: P50=0.085, P95=0.18, P99=0.32, P99.9=0.58 (vector constants)
- P99 improvement %: ((0.32 - histogram_quantile(0.99, rate(sfc_current_latency_seconds_bucket[5m]))) / 0.32) * 100
- Throughput <100ms: rate(sfc_requests_processed_total{latency_bucket="le_0.1"}[5m])
- Throughput improvement %: ((rate(sfc_requests_processed_total{latency_bucket="le_0.1"}[5m]) - 2100) / 2100) * 100
- Processing: rate(vnf_processing_latency_seconds_sum[5m]) / rate(vnf_processing_latency_seconds_count[5m])
- Queuing:    rate(vnf_queuing_latency_seconds_sum[5m]) / rate(vnf_queuing_latency_seconds_count[5m])
- Network:    rate(sdn_network_latency_seconds_sum[5m]) / rate(sdn_network_latency_seconds_count[5m])

Panels
- P95 vs baseline (timeseries), P95 improvement % (stat)
- P50/P95/P99/P99.9 (timeseries), P99 improvement % (stat)
- <100ms throughput (timeseries), throughput improvement % (stat)
- Component breakdown: processing/queuing/network (timeseries)

Regenerate JSON
- cd orchestration && python grafana_dashboards.py
- Outputs: orchestration/grafana/dashboards/ and dashboard_index.json
- PromQL refs: orchestration/grafana/queries/latency_improvement_promql.txt

2) Mininet Integration (Light Mode)
- Script: scripts/sfc_topology.py
- Flags: --light (2 hosts, 1 switch), --no-vnfs, --no-cli

Recommended commands
- Minimal: sudo -E python3 scripts/sfc_topology.py --light --no-vnfs --no-cli
- Light + CLI: sudo -E python3 scripts/sfc_topology.py --light --no-vnfs
- Full: sudo -E python3 scripts/sfc_topology.py

Notes
- If starting VNFs, ensure images: my-firewall-vnf, my-spamfilter-vnf, my-encryption-vnf, my-contentfilter-vnf
- When compose stack (Prometheus/Grafana/Controller) is running, prefer --light/--no-vnfs on 6 GB RAM

3) Environment, Dependencies, and Grafana Provisioning Updates
Date: 2025-10-01

What changed (summary)
- Upgraded orchestrator Docker images to Python 3.10-slim and added build tools.
- Switched pip installs to prefer prebuilt wheels and disabled cache to reduce disk usage.
- Relaxed version pins in orchestration/requirements.txt; aligned DL libs in root requirements.
- Added Grafana provisioning (datasource + dashboards) and mounted dashboards dir in compose.
- Extended README with disk space tips and resilient pip install commands.

Files updated
- orchestration/Dockerfile.orchestrator: base image -> python:3.10-slim; pip flags; build-essential.
- orchestration/Dockerfile.sdn: base image -> python:3.10-slim; pip flags; build-essential.
- orchestration/requirements.txt: relaxed pins (numpy>=1.24,<2; torch>=2.1,<2.4; etc.).
- requirements.txt (root): aligned torch/torchvision/torchaudio ranges; aiohttp/matplotlib ranges.
- orchestration/docker-compose.yml: mounted Grafana dashboards and provisioning directories.
- orchestration/grafana/provisioning/datasources/datasource.yml: default Prometheus datasource (http://prometheus:9090).
- orchestration/grafana/provisioning/dashboards/dashboards.yml: provider to auto-load dashboards from /var/lib/grafana/dashboards.
- README.md: added pip install guidance and disk cleanup steps (pip cache purge, docker prune, etc.).

Why
- Fix DRL crash ("Numpy is not available") by ensuring compatible wheels and stable env.
- Avoid SciPy/PyTorch build conflicts by relaxing pins and preferring wheels.
- Reduce "No space left on device" via no-cache installs and cleanup guidance.
- Ensure Grafana auto-loads dashboards and uses the correct Prometheus datasource.

How to apply
- Rebuild stack: docker compose -f orchestration/docker-compose.yml build --no-cache
- Start stack:  docker compose -f orchestration/docker-compose.yml up -d
- Orchestrator (host): python -m orchestration.integrated_system

4) Critical: Orchestrator 9091 Binding (ERR_CONNECTION_RESET)
- Symptom: Browser shows ERR_CONNECTION_RESET for http://localhost:9091; Grafana/Prometheus show "No data".
- Root cause: Flask in the orchestrator bound to 127.0.0.1 instead of 0.0.0.0, making it unreachable from containers/host.
- Fix: Ensure the Flask server starts with host=0.0.0.0 on port 9091.
  Python (example)
  app.run(host='0.0.0.0', port=9091, debug=False)
- Where to check: orchestration/vnf_orchestrator.py (or the server entry in the orchestrator process).
- How to run: From repo root, start orchestrator with: python -m orchestration.integrated_system
- Compose notes: Expose/map 9091 in orchestration/docker-compose.yml if orchestrator runs in Docker.
- Prometheus target: Use http://orchestrator:9091 from containers; use http://localhost:9091 from host.

5) orchestration_config.yml Not Found (Warning)
- Message: WARNING - Config file orchestration_config.yml not found, using defaults
- Expected path: orchestration/orchestration_config.yml
- Impact: Defaults are used for SFC definition and performance targets.
- If you need custom settings:
  1) Create/save exact file name orchestration_config.yml under orchestration/
  2) Populate your SFC and targets; then restart orchestrator (python -m orchestration.integrated_system)
- If defaults are fine: You can ignore the warning; dashboards and tests will still work using defaults.